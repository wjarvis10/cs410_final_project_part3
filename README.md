# cs410_final_project_part3

## Our Approach

When we began developing our FinalAgent, we started by researching how AlphaGo worked, thinking there's no need to re-invent the wheel. So we implemented MCTS using our policy and value networks, but quickly discovered that it was not performing well - its weakness seeming to be the lack of high quality value and policy networks. In order to fix this, we decided to gather more training data in hopes that it would enhance the Value and Policy Networkss; to do this, we tried to run self-play games in the Open-spiel environment and save the data to a pkl file using Open-spiel's SerializeGameAndState function. This proved to be extremely difficult - specifically in getting its format to perfectly align with how you guys saved the data in dataset_5x5.pkl file, so that when accessing it, it could be deserialized into (state, action, result) tuples. We eventually decided to abandon this approach due to time constraints.

Our next and final implementation was to use enhance our IDS alorgithm using the Value network we had trained from part 2. This proved to work rather well, and we decided to also implement a basic opening book strategy. We combined research of opening strategies with our own learned experience playing 5x5 go to determine which moves made for a good opening; we then experimented with variations of the opening book, specifically - the ranking of moves, the number of moves in the book,and the number of moves the algorithm would look to the book for possible moves. We eventually decided on our current implementation, as it perfomed the best - consistently beating our alphabeta algorthm.

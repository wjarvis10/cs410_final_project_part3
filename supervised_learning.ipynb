{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k35Swp-8KoKs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open-spiel in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (1.5)\n",
      "Requirement already satisfied: pip>=20.0.2 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (24.2)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (24.2.0)\n",
      "Requirement already satisfied: absl-py>=0.10.0 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.5 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (1.14.1)\n",
      "Requirement already satisfied: ml-collections>=0.1.1 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (1.0.0)\n",
      "Requirement already satisfied: six in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from ml-collections>=0.1.1->open-spiel) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from ml-collections>=0.1.1->open-spiel) (6.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/WillJarvis/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Needed if running on Colab\n",
    "!pip3 install open-spiel\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "O1yAh0sTKs3K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from go_search_problem import GoProblem, GoState\n",
    "from heuristic_go_problems import GoProblemLearnedHeuristic, GoProblemSimpleHeuristic\n",
    "from agents import GreedyAgent, RandomAgent, MCTSAgent, GameAgent, MinimaxAgent, AlphaBetaAgent, IterativeDeepeningAgent\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from game_runner import run_many\n",
    "import pickle\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6XNzHOq6QCQD"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path: str):\n",
    "    with open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "dataset_5x5 = load_dataset('dataset_5x5.pkl')\n",
    "# dataset_9x9 = load_dataset('9x9_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5fAQlSAOLXoj"
   },
   "outputs": [],
   "source": [
    "def save_model(path: str, model):\n",
    "    \"\"\"\n",
    "    Save model to a file\n",
    "    Input:\n",
    "        path: path to save model to\n",
    "        model: Pytorch model to save\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "def load_model(path: str, model):\n",
    "    \"\"\"\n",
    "    Load model from file\n",
    "\n",
    "    Note: you still need to provide a model (with the same architecture as the saved model))\n",
    "\n",
    "    Input:\n",
    "        path: path to load model from\n",
    "        model: Pytorch model to load\n",
    "    Output:\n",
    "        model: Pytorch model loaded from file\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOzaAXYrM4d3"
   },
   "source": [
    "# Task 1: Convert GameState to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1hbg6LkAMrZW"
   },
   "outputs": [],
   "source": [
    "def get_features(game_state: GoState):\n",
    "    \"\"\"\n",
    "    Map a game state to a list of features.\n",
    "\n",
    "    Some useful functions from game_state include:\n",
    "        game_state.size: size of the board\n",
    "        get_pieces_coordinates(player_index): get coordinates of all pieces of a player (0 or 1)\n",
    "        get_pieces_array(player_index): get a 2D array of pieces of a player (0 or 1)\n",
    "        \n",
    "        get_board(): get a 2D array of the board with 4 channels (player 0, player 1, empty, and player to move). 4 channels means the array will be of size 4 x n x n\n",
    "    \n",
    "        Descriptions of these methods can be found in the GoState\n",
    "\n",
    "    Input:\n",
    "        game_state: GoState to encode into a fixed size list of features\n",
    "    Output:\n",
    "        features: list of features\n",
    "    \"\"\"\n",
    "    board_size = game_state.size\n",
    "    # black_pieces = game_state.get_pieces_array(0).flatten()\n",
    "    # white_pieces = game_state.get_pieces_array(1).flatten()\n",
    "\n",
    "    # One-hot encoding of stone locations (n * n * 2 features)\n",
    "    # stone_locations = np.concatenate((black_pieces, white_pieces)).tolist()\n",
    "\n",
    "    # Player to move ( 0 = black, 1 = white)\n",
    "    # player_to_move = game_state.player_to_move()\n",
    "\n",
    "\n",
    "    # TODO: Encode game_state into a list of features\n",
    "    features = [] \n",
    "    # features.extend(board)\n",
    "    # features.append(player_to_move)\n",
    "    features = np.array(game_state.get_board()).flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2cpr86wH8W3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoState(komi=0.5, to_play=B, history.size()=0)\n",
      "\n",
      " 5 +++++\n",
      " 4 +++++\n",
      " 3 +++++\n",
      " 2 +++++\n",
      " 1 +++++\n",
      "   ABCDE\n",
      "\n",
      "Number of features:  100\n",
      "features [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "Action # 11\n",
      "Game Result -1.0\n"
     ]
    }
   ],
   "source": [
    "# Print information about first data point\n",
    "data_point = dataset_5x5[0]\n",
    "features = get_features(data_point[0])\n",
    "action = data_point[1]\n",
    "result = data_point[2]\n",
    "print(data_point[0])\n",
    "print(\"Number of features: \", len(features))\n",
    "print(\"features\", features)\n",
    "print(\"Action #\", action)\n",
    "print(\"Game Result\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI86jYgcOfHC"
   },
   "source": [
    "# Task 2: Supervised Learning of a Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "boPRx0o5Bqq9"
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "      super(ValueNetwork, self).__init__()\n",
    "\n",
    "      # TODO: What should the output size of a Value function be?\n",
    "      output_size = 1\n",
    "\n",
    "      # TODO: Add more layers, non-linear functions, etc.=\n",
    "      self.linear = nn.Linear(input_size, output_size)\n",
    "      \n",
    "      # self.fc1 = nn.Linear(input_size, 128)\n",
    "      # self.fc2 = nn.Linear(128, 64)\n",
    "      # self.fc3 = nn.Linear(64, 32)\n",
    "      # self.fc4 = nn.Linear(32, 16)\n",
    "      # self.fc5 = nn.Linear(16, output_size)\n",
    "      \n",
    "      self.fc1 = nn.Linear(input_size, 32)\n",
    "      self.fc2 = nn.Linear(32, 16)\n",
    "      self.fc3 = nn.Linear(16, 8)\n",
    "      self.fc4 = nn.Linear(8, output_size)\n",
    "\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "      self.tanh = nn.Tanh()\n",
    "      self.relu = nn.ReLU() \n",
    "      \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      \"\"\"\n",
    "      Run forward pass of network\n",
    "\n",
    "      Input:\n",
    "        x: input to network\n",
    "      Output:\n",
    "        output of network\n",
    "      \"\"\"\n",
    "      # TODO: Update as more layers are added\n",
    "\n",
    "      z1 = self.fc1(x)\n",
    "      a1 = self.relu(z1)\n",
    "      \n",
    "      z2 = self.fc2(a1)\n",
    "      a2 = self.tanh(z2)\n",
    "      \n",
    "      z3 = self.fc3(a2)\n",
    "      a3 = self.relu(z3)\n",
    "\n",
    "      z4 = self.fc4(a3)\n",
    "      # a4 = self.tanh(z4)\n",
    "      output = self.sigmoid(z4)\n",
    "\n",
    "      # z5 = self.fc5(a4)\n",
    "      # output = self.sigmoid(z5)\n",
    "\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "83a6vGLqB4E7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Value tensor([0.4719], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This will not produce meaningful outputs until trained, but you can test for syntax errors\n",
    "features_tensor = torch.Tensor(features)\n",
    "value_net = ValueNetwork(len(features))\n",
    "print(\"predicted Value\", value_net(features_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "id": "Rq8CokTvOyrI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Loss:  tensor(4.2536, grad_fn=<AddBackward0>)\n",
      "Epoch:  1\n",
      "Loss:  tensor(4.0805, grad_fn=<AddBackward0>)\n",
      "Epoch:  2\n",
      "Loss:  tensor(4.0346, grad_fn=<AddBackward0>)\n",
      "Epoch:  3\n",
      "Loss:  tensor(4.0134, grad_fn=<AddBackward0>)\n",
      "Epoch:  4\n",
      "Loss:  tensor(4.0152, grad_fn=<AddBackward0>)\n",
      "Epoch:  5\n",
      "Loss:  tensor(4.0229, grad_fn=<AddBackward0>)\n",
      "Epoch:  6\n",
      "Loss:  tensor(4.0310, grad_fn=<AddBackward0>)\n",
      "Epoch:  7\n",
      "Loss:  tensor(4.0374, grad_fn=<AddBackward0>)\n",
      "Epoch:  8\n",
      "Loss:  tensor(4.0384, grad_fn=<AddBackward0>)\n",
      "Epoch:  9\n",
      "Loss:  tensor(4.0453, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def train_value_network(dataset, num_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a value network on the provided dataset.\n",
    "\n",
    "    Input:\n",
    "        dataset: list of (state, action, result) tuples\n",
    "        num_epochs: number of epochs to train for\n",
    "        learning_rate: learning rate for gradient descent\n",
    "    Output:\n",
    "        model: trained model\n",
    "    \"\"\"\n",
    "    # Make sure dataset is shuffled for better performance\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    # You may find it useful to create train/test sets to better track performance/overfit/underfit\n",
    "    # train_size = int(0.8 * len(dataset))\n",
    "    # train_data = dataset[:train_size]\n",
    "    # test_data = dataset[train_size:]\n",
    "\n",
    "    # TODO: Create model\n",
    "    input_size = len(get_features(dataset[0][0]))\n",
    "    model = ValueNetwork(input_size)\n",
    "\n",
    "    # TODO: Specify Loss Function - use MSE bc continuous outcome value\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # You can use Adam, which is stochastic gradient descent with ADAptive Momentum\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        batch_loss = 0.0\n",
    "        batch_counter = 0\n",
    "\n",
    "        # state = (state, action, outcome)\n",
    "        for data_point in dataset:\n",
    "            state = data_point[0]\n",
    "            features = get_features(state)\n",
    "            features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "            # TODO: What should the desired output of the value network be?\n",
    "            # Note: You will have to convert the label to a torch tensor to use with torch's loss functions\n",
    "            outcome = data_point[2]\n",
    "            label = torch.tensor(outcome)\n",
    "\n",
    "            # TODO: Get model prediction of value\n",
    "            prediction = model(features_tensor)\n",
    "\n",
    "            # TODO: Compute Loss for data point\n",
    "            loss = loss_function(prediction, label)\n",
    "            batch_loss += loss\n",
    "            batch_counter += 1\n",
    "\n",
    "            if batch_counter % batch_size == 0:\n",
    "                # Call backward to run backward pass and compute gradients\n",
    "                batch_loss.backward()\n",
    "\n",
    "                # Run gradient descent step with optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reset gradient for next batch\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss = 0.0\n",
    "        print(\"Epoch: \", epoch)\n",
    "        print(\"Loss: \", batch_loss)\n",
    "    return model\n",
    "\n",
    "value_model = train_value_network(dataset_5x5, 10, 1e-4)\n",
    "save_model(\"value_model_2.pt\", value_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekP8mzDaTOUM"
   },
   "source": [
    "## Comparing Learned Value function against other Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "UWl3dLOnTbiD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax Agent GreedyAgent + Simple Heuristic\n",
      "Learned Minimax Agent GreedyAgent + Learned Heuristic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1: GreedyAgent + Learned Heuristic Score: 100.0\n",
      "Agent 2: GreedyAgent + Simple Heuristic Score: -100.0\n",
      "Agent 1: GreedyAgent + Learned Heuristic Score with Black (first move): 50.0\n",
      "Agent 2: GreedyAgent + Simple Heuristic Score with Black (first move): -50.0\n",
      "Agent 1: GreedyAgent + Learned Heuristic Average Duration: 0.001162473041132876\n",
      "Agent 2: GreedyAgent + Simple Heuristic Average Duration: 0.00035214046679045045\n",
      "Agent 1: GreedyAgent + Learned Heuristic Longest Duration: 0.003551006317138672\n",
      "Agent 2: GreedyAgent + Simple Heuristic Longest Duration: 0.0010340213775634766\n",
      "Agent 1: GreedyAgent + Learned Heuristic Average Time Remaining: 36.97482639074325\n",
      "Agent 2: GreedyAgent + Simple Heuristic Average Time Remaining: 36.99227107048036\n",
      "Agent 1: GreedyAgent + Learned Heuristic Min Time Remaining: 33.97310280799866\n",
      "Agent 2: GreedyAgent + Simple Heuristic Min Time Remaining: 33.99274182319641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100.0, -100.0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GoProblemLearnedHeuristic(GoProblem):\n",
    "    def __init__(self, model=None, state=None):\n",
    "        super().__init__(state=state)\n",
    "        self.model = model\n",
    "        \n",
    "    def __call__(self, model=None):\n",
    "        \"\"\"\n",
    "        Use the model to compute a heuristic value for a given state.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def encoding(self, state):\n",
    "        \"\"\"\n",
    "        Get encoding of state (convert state to features)\n",
    "        Note, this may call get_features() from Task 1. \n",
    "\n",
    "        Input:\n",
    "            state: GoState to encode into a fixed size list of features\n",
    "        Output:\n",
    "            features: list of features\n",
    "        \"\"\"\n",
    "        # TODO: get encoding of state (convert state to features)\n",
    "        features = get_features(state)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def heuristic(self, state, player_index):\n",
    "        \"\"\"\n",
    "        Return heuristic (value) of current state\n",
    "\n",
    "        Input:\n",
    "            state: GoState to encode into a fixed size list of features\n",
    "            player_index: index of player to evaluate heuristic for\n",
    "        Output:\n",
    "            value: heuristic (value) of current state\n",
    "        \"\"\"\n",
    "        # TODO: Compute heuristic (value) of current state\n",
    "        features = self.encoding(state)\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        value = self.model(features_tensor)\n",
    "        \n",
    "        # if player_index == 1:\n",
    "        #     value = -value\n",
    "\n",
    "        # Note, your agent may perform better if you force it not to pass\n",
    "        # (i.e., don't select action #25 on a 5x5 board unless necessary)\n",
    "        return value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Learned Heuristic\"\n",
    "\n",
    "\n",
    "def create_value_agent_from_model():\n",
    "    \"\"\"\n",
    "    Create agent object from saved model. This (or other methods like this) will be how your agents will be created in gradescope and in the final tournament.\n",
    "    \"\"\"\n",
    "\n",
    "    model_path = \"value_model.pt\"\n",
    "    # TODO: Update number of features for your own encoding size\n",
    "    feature_size =  len(features)\n",
    "    model = load_model(model_path, ValueNetwork(feature_size))\n",
    "    heuristic_search_problem = GoProblemLearnedHeuristic(model)\n",
    "\n",
    "    # TODO: Try with other heuristic agents (IDS/AB/Minimax)\n",
    "    learned_agent = GreedyAgent(heuristic_search_problem)\n",
    "\n",
    "    return learned_agent\n",
    "\n",
    "learned_agent = create_value_agent_from_model()\n",
    "agent2 = GreedyAgent()\n",
    "print(\"Minimax Agent\", agent2)\n",
    "print(\"Learned Minimax Agent\", learned_agent)\n",
    "\n",
    "run_many(learned_agent, agent2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUUOOIYhRjoT"
   },
   "source": [
    "# Task 3: Supervised Learning of a Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "id": "dHgeNqBeBm3b"
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, board_size=5):\n",
    "      super(PolicyNetwork, self).__init__()\n",
    "\n",
    "      # TODO: What should the output size of the Policy be?\n",
    "      output_size = (board_size * board_size) + 1\n",
    "\n",
    "      # TODO: Add more layers, non-linear functions, etc.\n",
    "      self.linear = nn.Linear(input_size, output_size)\n",
    "      \n",
    "      self.fc1 = nn.Linear(input_size, 32)\n",
    "      self.fc2 = nn.Linear(32, 16)\n",
    "      self.fc3 = nn.Linear(16, 8)\n",
    "      self.fc4 = nn.Linear(8, output_size)\n",
    "\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "      self.tanh = nn.Tanh()\n",
    "      self.relu = nn.ReLU() \n",
    "      self.softmax = nn.Softmax(dim=-1) \n",
    "      # needed for class probabilities; output probabilities sum to 1\n",
    "\n",
    "    def forward(self, x):\n",
    "      # TODO: Update as more layers are added\n",
    "\n",
    "      z1 = self.fc1(x)\n",
    "      a1 = self.relu(z1)\n",
    "      \n",
    "      z2 = self.fc2(a1)\n",
    "      a2 = self.relu(z2)\n",
    "      \n",
    "      z3 = self.fc3(a2)\n",
    "      a3 = self.relu(z3)\n",
    "\n",
    "      z4 = self.fc4(a3)\n",
    "      output = self.sigmoid(z4)\n",
    "\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "id": "toR5q6qrBvUI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Action Probabilities tensor([0.5614, 0.5655, 0.4702, 0.4453, 0.4937, 0.5058, 0.4893, 0.5743, 0.5265,\n",
      "        0.5309, 0.5540, 0.4538, 0.4300, 0.4554, 0.5445, 0.5680, 0.4971, 0.4439,\n",
      "        0.5606, 0.5611, 0.5164, 0.4933, 0.5708, 0.5643, 0.5555, 0.4258],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This will not produce meaningful outputs until trained, but you can test for syntax errors\n",
    "features_tensor = torch.Tensor(features)\n",
    "policy_net = PolicyNetwork(len(features))\n",
    "print(\"Predicted Action Probabilities\", policy_net(features_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "id": "G6-P_g6wRi-Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Loss:  tensor(2.3527, grad_fn=<NllLossBackward0>)\n",
      "Epoch:  1\n",
      "Loss:  tensor(2.3630, grad_fn=<NllLossBackward0>)\n",
      "Epoch:  2\n",
      "Loss:  tensor(2.4049, grad_fn=<NllLossBackward0>)\n",
      "Epoch:  3\n",
      "Loss:  tensor(2.4167, grad_fn=<NllLossBackward0>)\n",
      "Epoch:  4\n",
      "Loss:  tensor(2.4200, grad_fn=<NllLossBackward0>)\n",
      "Epoch:  5\n",
      "Loss:  tensor(2.4192, grad_fn=<NllLossBackward0>)\n",
      "Epoch:  6\n",
      "Loss:  tensor(2.4185, grad_fn=<NllLossBackward0>)\n",
      "Epoch:  7\n",
      "Loss:  tensor(2.4185, grad_fn=<NllLossBackward0>)\n",
      "Epoch:  8\n",
      "Loss:  tensor(2.4170, grad_fn=<NllLossBackward0>)\n",
      "Epoch:  9\n",
      "Loss:  tensor(2.4027, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def train_policy_network(dataset, num_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a policy network on the provided dataset.\n",
    "\n",
    "    Input:\n",
    "        dataset: list of (state, action, result) tuples\n",
    "        num_epochs: number of epochs to train for\n",
    "        learning_rate: learning rate for gradient descent\n",
    "    Output:\n",
    "        model: trained model\n",
    "    \"\"\"\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    # TODO: Create model\n",
    "    feature_size =  len(get_features(dataset[0][0]))\n",
    "    model = PolicyNetwork(feature_size)\n",
    "\n",
    "    # TODO: Specify Loss Function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # You can use Adam, which is stochastic gradient descent with ADAptive Momentum\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # batch_loss = 0.0\n",
    "        # batch_counter = 0\n",
    "\n",
    "        # data = (state, action, outcome)\n",
    "        for data_point in dataset:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # TODO: Get features from state and convert features to torch tensor\n",
    "            state = data_point[0]\n",
    "            action = data_point[1]\n",
    "\n",
    "            features = get_features(state)\n",
    "            features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "            # TODO: What should the desired output of the value network be?\n",
    "            # Note: You will have to convert the label to a torch tensor to use with torch's loss functions\n",
    "            label = torch.tensor(action)\n",
    "\n",
    "            # TODO: Get model estimate of value\n",
    "            prediction = model(features_tensor)\n",
    "\n",
    "            # TODO: Compute Loss for data point\n",
    "            loss = loss_function(prediction, label)\n",
    "            # batch_loss += loss\n",
    "            # batch_counter += 1\n",
    "\n",
    "            # if batch_counter % batch_size == 0:\n",
    "            #     # Call backward to run backward pass and compute gradients\n",
    "            #     batch_loss.backward()\n",
    "\n",
    "            #     # Run gradient descent step with optimizer\n",
    "            #     optimizer.step()\n",
    "\n",
    "            #     # print(\"Batch_Loss: \", batch_loss)\n",
    "            #     optimizer.zero_grad()\n",
    "            #     batch_loss = 0.0\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch: \", epoch)\n",
    "        print(\"Loss: \", loss)\n",
    "        \n",
    "    return model\n",
    "\n",
    "policy_net = train_policy_network(dataset_5x5, 10, 1e-4)\n",
    "save_model(\"policy_model_3.pt\", policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU3ZxNNi-gWc"
   },
   "source": [
    "## Comparing Learned Policy against other Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "id": "UNqXigG6-dHG"
   },
   "outputs": [],
   "source": [
    "class PolicyAgent(GameAgent):\n",
    "    def __init__(self, search_problem, model_path, board_size=5):\n",
    "        super().__init__()\n",
    "        self.search_problem = search_problem\n",
    "        # self.model = load_model(model_path, PolicyNetwork)\n",
    "        self.board_size = board_size\n",
    "\n",
    "        feature_size = board_size ** 2 * 4\n",
    "        self.model = load_model(model_path, PolicyNetwork(feature_size, board_size))\n",
    "        self.model.eval()\n",
    "\n",
    "    def encoding(self, state):\n",
    "        # TODO: get encoding of state (convert state to features)\n",
    "        features = get_features(state)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_move(self, game_state, time_limit=1):\n",
    "      \"\"\"\n",
    "      Get best action for current state using self.model\n",
    "\n",
    "      Input:\n",
    "        game_state: current state of the game\n",
    "        time_limit: time limit for search (This won't be used in this agent)\n",
    "      Output:\n",
    "        action: best action to take\n",
    "      \"\"\"\n",
    "\n",
    "      # TODO: Select LEGAL Best Action predicted by model\n",
    "\n",
    "      features = self.encoding(game_state)\n",
    "      features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "      \n",
    "      probabilities = self.model(features_tensor)\n",
    "      legal_actions = self.search_problem.get_available_actions(game_state)\n",
    "\n",
    "      # The top prediction of your model may not be a legal move!\n",
    "      best_action = random.choice(legal_actions)\n",
    "      action_probabilities = {}\n",
    "\n",
    "      for action in legal_actions:\n",
    "          action_probabilities[action] = probabilities[action]\n",
    "\n",
    "      best_prob = float('-inf') \n",
    "\n",
    "      for action, prob in action_probabilities.items(): \n",
    "          if prob > best_prob: \n",
    "              best_prob = prob\n",
    "              best_action = action\n",
    "\n",
    "      if best_action == 25: \n",
    "          action_probabilities[best_action] -= .2\n",
    "\n",
    "      for action, prob in action_probabilities.items(): \n",
    "          if prob > best_prob: \n",
    "              best_prob = prob\n",
    "              best_action = action\n",
    "\n",
    "      # Note, you may want to force your policy not to pass their turn unless necessary\n",
    "      assert best_action in self.search_problem.get_available_actions(game_state)\n",
    "      \n",
    "      return best_action\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Policy Agent\"\n",
    "    \n",
    "def create_policy_agent_from_model():\n",
    "    \"\"\"\n",
    "    Create agent object from saved model. This (or other methods like this) will be how your agents will be created in gradescope and in the final tournament.    \n",
    "    \"\"\"\n",
    "\n",
    "    model_path = \"policy_model.pt\"\n",
    "    agent = PolicyAgent(GoProblem(size=5), model_path)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "id": "8j6tGngt_LVu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Agent Policy Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 74.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1: Policy Agent Score: 40.0\n",
      "Agent 2: GreedyAgent + Simple Heuristic Score: -40.0\n",
      "Agent 1: Policy Agent Score with Black (first move): 20.0\n",
      "Agent 2: GreedyAgent + Simple Heuristic Score with Black (first move): -20.0\n",
      "Agent 1: Policy Agent Average Duration: 0.00011111008269446236\n",
      "Agent 2: GreedyAgent + Simple Heuristic Average Duration: 0.0002316193511852852\n",
      "Agent 1: Policy Agent Longest Duration: 0.0013790130615234375\n",
      "Agent 2: GreedyAgent + Simple Heuristic Longest Duration: 0.0014269351959228516\n",
      "Agent 1: Policy Agent Average Time Remaining: 33.997926092147814\n",
      "Agent 2: GreedyAgent + Simple Heuristic Average Time Remaining: 33.49577408432961\n",
      "Agent 1: Policy Agent Min Time Remaining: 28.996105194091797\n",
      "Agent 2: GreedyAgent + Simple Heuristic Min Time Remaining: 27.995012283325195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40.0, -40.0)"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy_agent = PolicyAgent(GoProblem(size=5), 'policy_model.pt')\n",
    "policy_agent = create_policy_agent_from_model()\n",
    "print(\"Policy Agent\", policy_agent)\n",
    "run_many(policy_agent, GreedyAgent(), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z2azdVNBxYS"
   },
   "source": [
    "# Submitting\n",
    "\n",
    "After you've completed all the tasks in this notebook, you'll want to add your agents to your agents.py file. You'll want to copy the necessary function and class definitions for PolicyAgent, GoProblemLearnedHeuristic, PolicyNetwork, ValueNetwork, and any other methods you referenced. Your agents will ultimately be tested on gradescope by calling create_value_agent_from_model or by create_policy_agent_from_model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs410_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
